{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80974a15-32a4-4212-aeee-ac6fd6368779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['pdf_pipeline']\n",
    "collection = db['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7607d2c1-484a-4f3f-bfef-0dd64fd9021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rajkumar\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rajkumar\n",
      "[nltk_data]     Gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b49522-755e-42bd-b1cb-68a9c5baacfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(sentence,word):\n",
    "    words=word_tokenize(sentence.lower())\n",
    "    return words.count(word.lower())/len(words)\n",
    "\n",
    "def compute_idf(word,corpus):\n",
    "    num_sentences=len(corpus)\n",
    "    num_sentences_with_word=sum(1 for sentence in corpus if word in word_tokenize(sentence.lower()))\n",
    "    return math.log((sum_sentences+1)/(num_sentences_with_word+1))+1\n",
    "\n",
    "def summarize_text(text,summary_length=3):\n",
    "    corpus=sent_tokenize(text)\n",
    "    tf_idf_score={}\n",
    "    for sentence in corpus:\n",
    "        sentence_score=0\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word not in stopword.words('english'):\n",
    "                tf=compute_tf(sentence,word)\n",
    "                idf=compute_idf(word,corpus)\n",
    "                sentence_score+=tf*idf\n",
    "        tf_idf_score[sentence]=sentence_score\n",
    "        top_sentences=sorted(tf_idf_score,key=tf_idf_score.get,reverse=True)[:summary_length]\n",
    "        return ' '.join(top_sentences)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b3a368b-5beb-4cd1-982f-a965ea47ffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27f70021-32ce-456b-a7f2-11a8ebe274e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text,num_keywords=5):\n",
    "    word=[word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stopword.words('english')]\n",
    "    unigram=counter(word)\n",
    "    bigram=counter([' '.join(gram) for gram in ngrams(word,2)])\n",
    "    combined=unigram+bigram\n",
    "    keywords=[word for word,freq in combined.most_common(num_keywords)]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a594e7b3-c88c-4c71-b8b8-f81910bddf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "def extract_text_from_pdf(filepath):\n",
    "    try:\n",
    "        extract_text(filepath)\n",
    "    except Exception as e:\n",
    "        print(f'error extracting text from {filepath}:{e}')\n",
    "        return \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc99b717-2300-47dd-a105-b48c31e09b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "class pdfhandler(FileSystemEventHandler):\n",
    "    def on_create(self,event):\n",
    "        if event.src_path.endswith('.pdf'):\n",
    "            print(f'pdf detected:{event.src_path}')\n",
    "            process_pdf(event.src_path)\n",
    "def monitor_folder(folder_path):\n",
    "    observer=Observer()\n",
    "    observer.schedule(pdfhandler(),folder_path,recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00dfae7-23e5-42cd-acf3-9edab08cac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observer started. Waiting for file events...\n"
     ]
    }
   ],
   "source": [
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import time\n",
    "\n",
    "class TestHandler(FileSystemEventHandler):\n",
    "    def on_created(self, event):\n",
    "        print(f\"File detected: {event.src_path}\")\n",
    "\n",
    "folder_to_watch = \"D:\\\\pdf_folder\"\n",
    "\n",
    "observer = Observer()\n",
    "observer.schedule(TestHandler(), folder_to_watch, recursive=False)\n",
    "observer.start()\n",
    "\n",
    "print(\"Observer started. Waiting for file events...\")\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    observer.stop()\n",
    "\n",
    "observer.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de700b-970e-4b7a-98b4-718713c7cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def process_pdf(filepath):\n",
    "    text=extract_text_from_pdf(filepath)\n",
    "    if not text:\n",
    "        return\n",
    "    summary=summarize_text(text)\n",
    "    keywords=extract_keywords(text)\n",
    "    document={\n",
    "        'file_name':os.path.basename(filepath),\n",
    "        'file_path':filepath,\n",
    "        'keywords':keywords,\n",
    "        'summary':summary,\n",
    "        'processed_at': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    \n",
    "    }\n",
    "    collection.insert_one(document)\n",
    "    print(f'stored document:{os.path.basename(filepath)} in MongoDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2975a-051f-409d-bb40-a3cb8eb9b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    folder_to_watch=\"D:\\\\pdf_folder\"\n",
    "    print(f\"Monitoring folder: {folder_to_watch}\")\n",
    "    monitor_folder(folder_to_watch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c189271-a33a-4a9a-9a88-456af6cfc6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34479f6-35a5-48de-85c2-930f97527eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
